<?xml version="1.0" encoding="utf-8"?>
  <section xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  version="5.0"
  xml:id="Container-to-Container-Synchronization">
<title>Container to Container Synchronization</title>
<section xml:id="overview">
  <title>Overview</title>
<section xml:id="page-abstract">
<title>Page Abstract</title>
<para>    Swift has a feature where all the contents of a container can be
    mirrored to another container through background synchronization.
    Swift cluster operators configure their cluster to allow/accept sync
    requests to/from other clusters, and the user specifies where to
    sync their container to along with a secret synchronization key.
  </para>
  <blockquote>
    <para>
      <emphasis role="strong">note</emphasis>
    </para>
    <para>
      Container sync will sync object POSTs only if the proxy server is
      set to use &quot;object_post_as_copy = true&quot; which is the
      default. So-called fast object posts, &quot;object_post_as_copy =
      false&quot; do not update the container listings and therefore
      can't be detected for synchronization.
    </para>
  </blockquote>
  <blockquote>
    <para>
      <emphasis role="strong">note</emphasis>
    </para>
    <para>
      If you are using the large objects feature you will need to ensure
      both your manifest file and your segment files are synced if they
      happen to be in different containers.
    </para>
  </blockquote>
</sect1>
</section>
<section xml:id="configuring-a-clusters-allowable-sync-hosts">
  <title>Configuring a Cluster's Allowable Sync Hosts</title>
  <para>
    The Swift cluster operator must allow synchronization with a set of
    hosts before the user can enable container synchronization. First,
    the backend container server needs to be given this list of hosts in
    the container-server.conf file:
  </para>
  <programlisting>
[DEFAULT]
# This is a comma separated list of hosts allowed in the
# X-Container-Sync-To field for containers.
# allowed_sync_hosts = 127.0.0.1
allowed_sync_hosts = host1,host2,etc.
...

[container-sync]
# You can override the default log routing for this app here (don't
# use set!):
# log_name = container-sync
# log_facility = LOG_LOCAL0
# log_level = INFO
# Will sync, at most, each container once per interval
# interval = 300
# Maximum amount of time to spend syncing each container
# container_time = 60
</programlisting>
  <para>
    Tracking sync progress, problems, and just general activity can only
    be achieved with log processing for this first release of container
    synchronization. In that light, you may wish to set the above log_
    options to direct the container-sync logs to a different file for
    easier monitoring. Additionally, it should be noted there is no way
    for an end user to detect sync progress or problems other than
    HEADing both containers and comparing the overall information.
  </para>
</section>
<section xml:id="using-the-swift-tool-to-set-up-synchronized-containers">
  <title>Using the <literal>swift</literal> tool to set up synchronized
  containers</title>
  <blockquote>
    <para>
      <emphasis role="strong">note</emphasis>
    </para>
    <para>
      The <literal>swift</literal> tool is available from the
      <link xlink:href="http://github.com/openstack/python-swiftclient">python-swiftclient</link>
      library.
    </para>
  </blockquote>
  <blockquote>
    <para>
      <emphasis role="strong">note</emphasis>
    </para>
    <para>
      You must be the account admin on the account to set
      synchronization targets and keys.
    </para>
  </blockquote>
  <para>
    You simply tell each container where to sync to and give it a secret
    synchronization key. First, let's get the account details for our
    two cluster accounts:
  </para>
  <programlisting>
$ swift -A http://cluster1/auth/v1.0 -U test:tester -K testing stat -v
StorageURL: http://cluster1/v1/AUTH_208d1854-e475-4500-b315-81de645d060e
Auth Token: AUTH_tkd5359e46ff9e419fa193dbd367f3cd19
   Account: AUTH_208d1854-e475-4500-b315-81de645d060e
Containers: 0
   Objects: 0
     Bytes: 0

$ swift -A http://cluster2/auth/v1.0 -U test2:tester2 -K testing2 stat -v
StorageURL: http://cluster2/v1/AUTH_33cdcad8-09fb-4940-90da-0f00cbf21c7c
Auth Token: AUTH_tk816a1aaf403c49adb92ecfca2f88e430
   Account: AUTH_33cdcad8-09fb-4940-90da-0f00cbf21c7c
Containers: 0
   Objects: 0
     Bytes: 0
</programlisting>
  <para>
    Now, let's make our first container and tell it to synchronize to a
    second we'll make next:
  </para>
  <programlisting>
$ swift -A http://cluster1/auth/v1.0 -U test:tester -K testing post \
  -t 'http://cluster2/v1/AUTH_33cdcad8-09fb-4940-90da-0f00cbf21c7c/container2' \
  -k 'secret' container1
</programlisting>
  <para>
    The <literal>-t</literal> indicates the URL to sync to, which is the
    <literal>StorageURL</literal> from cluster2 we retrieved above plus
    the container name. The <literal>-k</literal> specifies the secret
    key the two containers will share for synchronization. Now, we'll do
    something similar for the second cluster's container:
  </para>
  <programlisting>
$ swift -A http://cluster2/auth/v1.0 -U test2:tester2 -K testing2 post \
  -t 'http://cluster1/v1/AUTH_208d1854-e475-4500-b315-81de645d060e/container1' \
  -k 'secret' container2
</programlisting>
  <para>
    That's it. Now we can upload a bunch of stuff to the first container
    and watch as it gets synchronized over to the second:
  </para>
  <programlisting>
$ swift -A http://cluster1/auth/v1.0 -U test:tester -K testing \
  upload container1 .
photo002.png
photo004.png
photo001.png
photo003.png

$ swift -A http://cluster2/auth/v1.0 -U test2:tester2 -K testing2 \
  list container2

[Nothing there yet, so we wait a bit...]
[If you're an operator running SAIO and just testing, you may need to
 run 'swift-init container-sync once' to perform a sync scan.]

$ swift -A http://cluster2/auth/v1.0 -U test2:tester2 -K testing2 \
  list container2
photo001.png
photo002.png
photo003.png
photo004.png
</programlisting>
  <para>
    You can also set up a chain of synced containers if you want more
    than two. You'd point 1 -&gt; 2, then 2 -&gt; 3, and finally 3 -&gt;
    1 for three containers. They'd all need to share the same secret
    synchronization key.
  </para>
</section>
<section xml:id="using-curl-or-other-tools-instead">
  <title>Using curl (or other tools) instead</title>
  <para>
    So what's <literal>swift</literal> doing behind the scenes? Nothing
    overly complicated. It translates the
    <literal>-t &lt;value&gt;</literal> option into an
    <literal>X-Container-Sync-To: &lt;value&gt;</literal> header and the
    <literal>-k &lt;value&gt;</literal> option into an
    <literal>X-Container-Sync-Key: &lt;value&gt;</literal> header.
  </para>
  <para>
    For instance, when we created the first container above and told it
    to synchronize to the second, we could have used this curl command:
  </para>
  <programlisting>
$ curl -i -X POST -H 'X-Auth-Token: AUTH_tkd5359e46ff9e419fa193dbd367f3cd19' \
  -H 'X-Container-Sync-To: http://cluster2/v1/AUTH_33cdcad8-09fb-4940-90da-0f00cbf21c7c/container2' \
  -H 'X-Container-Sync-Key: secret' \
  'http://cluster1/v1/AUTH_208d1854-e475-4500-b315-81de645d060e/container1'
HTTP/1.1 204 No Content
Content-Length: 0
Content-Type: text/plain; charset=UTF-8
Date: Thu, 24 Feb 2011 22:39:14 GMT
</programlisting>
</section>
<section xml:id="whats-going-on-behind-the-scenes-in-the-cluster">
  <title>What's going on behind the scenes, in the cluster?</title>
  <para>
    The swift-container-sync does the job of sending updates to the
    remote container.
  </para>
  <para>
    This is done by scanning the local devices for container databases
    and checking for x-container-sync-to and x-container-sync-key
    metadata values. If they exist, newer rows since the last sync will
    trigger PUTs or DELETEs to the other container.
  </para>
  <blockquote>
    <para>
      <emphasis role="strong">note</emphasis>
    </para>
    <para>
      The swift-container-sync process runs on each container server in
      the cluster and talks to the proxy servers in the remote cluster.
      Therefore, the container servers must be permitted to initiate
      outbound connections to the remote proxy servers.
    </para>
  </blockquote>
  <blockquote>
    <para>
      <emphasis role="strong">note</emphasis>
    </para>
    <para>
      Container sync will sync object POSTs only if the proxy server is
      set to use &quot;object_post_as_copy = true&quot; which is the
      default. So-called fast object posts, &quot;object_post_as_copy =
      false&quot; do not update the container listings and therefore
      can't be detected for synchronization.
    </para>
  </blockquote>
  <para>
    The actual syncing is slightly more complicated to make use of the
    three (or number-of-replicas) main nodes for a container without
    each trying to do the exact same work but also without missing work
    if one node happens to be down.
  </para>
  <para>
    Two sync points are kept in each container database. When syncing a
    container, the container-sync process figures out which replica of
    the container it has. In a standard 3-replica scenario, the process
    will have either replica number 0, 1, or 2. This is used to figure
    out which rows are belong to this sync process and which ones don't.
  </para>
  <para>
    An example may help. Assume a replica count of 3 and database row
    IDs are 1..6. Also, assume that container-sync is running on this
    container for the first time, hence SP1 = SP2 = -1. :
  </para>
  <programlisting>
SP1
SP2
 |
 v
-1 0 1 2 3 4 5 6
</programlisting>
  <para>
    First, the container-sync process looks for rows with id between SP1
    and SP2. Since this is the first run, SP1 = SP2 = -1, and there
    aren't any such rows. :
  </para>
  <programlisting>
SP1
SP2
 |
 v
-1 0 1 2 3 4 5 6
</programlisting>
  <para>
    Second, the container-sync process looks for rows with id greater
    than SP1, and syncs those rows which it owns. Ownership is based on
    the hash of the object name, so it's not always guaranteed to be
    exactly one out of every three rows, but it usually gets close. For
    the sake of example, let's say that this process ends up owning rows
    2 and 5.
  </para>
  <para>
    Once it's finished trying to sync those rows, it updates SP1 to be
    the biggest row-id that it's seen, which is 6 in this example. :
  </para>
  <programlisting>
SP2           SP1
 |             |
 v             v
-1 0 1 2 3 4 5 6
</programlisting>
  <para>
    While all that was going on, clients uploaded new objects into the
    container, creating new rows in the database. :
  </para>
  <programlisting>
SP2           SP1
 |             |
 v             v
-1 0 1 2 3 4 5 6 7 8 9 10 11 12
</programlisting>
  <para>
    On the next run, the container-sync starts off looking at rows with
    ids between SP1 and SP2. This time, there are a bunch of them. The
    sync process try to sync all of them. If it succeeds, it will set
    SP2 to equal SP1. If it fails, it will set SP2 to the failed object
    and will continue to try all other objects till SP1, setting SP2 to
    the first object that failed.
  </para>
  <para>
    Under normal circumstances, the container-sync processes will have
    already taken care of synchronizing all rows, between SP1 and SP2,
    resulting in a set of quick checks. However, if one of the sync
    processes failed for some reason, then this is a vital fallback to
    make sure all the objects in the container get synchronized. Without
    this seemingly-redundant work, any container-sync failure results in
    unsynchronized objects. Note that the container sync will
    persistently retry to sync any faulty object until success, while
    logging each failure.
  </para>
  <para>
    Once it's done with the fallback rows, and assuming no faults
    occured, SP2 is advanced to SP1. :
  </para>
  <programlisting>
SP2
SP1
 |
 v
</programlisting>
  <blockquote>
    <para>
      -1 0 1 2 3 4 5 6 7 8 9 10 11 12
    </para>
  </blockquote>
  <para>
    Then, rows with row ID greater than SP1 are synchronized (provided
    this container-sync process is responsible for them), and SP1 is
    moved up to the greatest row ID seen. :
  </para>
  <programlisting>
SP2            SP1
 |              |
 v              v
</programlisting>
  <blockquote>
    <para>
      -1 0 1 2 3 4 5 6 7 8 9 10 11 12
    </para>
  </blockquote>
</section>
</section>
