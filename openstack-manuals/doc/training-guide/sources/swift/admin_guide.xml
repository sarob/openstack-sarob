<?xml version="1.0" encoding="utf-8"?>
  <section xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  version="5.0"
  xml:id="Administrator's-Guide">
<title>Administrator's Guide</title>
<section xml:id="managing-the-rings">
  <title>Managing the Rings</title>
<section xml:id="page-abstract">
<title>Page Abstract</title>
<para>    You may build the storage rings on any server with the appropriate
    version of Swift installed. Once built or changed (rebalanced), you
    must distribute the rings to all the servers in the cluster. Storage
    rings contain information about all the Swift storage partitions and
    how they are distributed between the different nodes and disks.
  </para>
  <para>
    Swift 1.6.0 is the last version to use a Python pickle format.
    Subsequent versions use a different serialization format.
    <emphasis role="strong">Rings generated by Swift versions 1.6.0 and
    earlier may be read by any version, but rings generated after 1.6.0
    may only be read by Swift versions greater than 1.6.0.</emphasis> So
    when upgrading from version 1.6.0 or earlier to a version greater
    than 1.6.0, either upgrade Swift on your ring building server
    <emphasis role="strong">last</emphasis> after all Swift nodes have
    been successfully upgraded, or refrain from generating rings until
    all Swift nodes have been successfully upgraded.
  </para>
  <para>
    If you need to downgrade from a version of swift greater than 1.6.0
    to a version less than or equal to 1.6.0, first downgrade your
    ring-building server, generate new rings, push them out, then
    continue with the rest of the downgrade.
  </para>
  <para>
    For more information see overview_ring.
  </para>
  <para>
    Removing a device from the ring:
  </para>
  <programlisting>
swift-ring-builder &lt;builder-file&gt; remove &lt;ip_address&gt;/&lt;device_name&gt;
</programlisting>
  <para>
    Removing a server from the ring:
  </para>
  <programlisting>
swift-ring-builder &lt;builder-file&gt; remove &lt;ip_address&gt;
</programlisting>
  <para>
    Adding devices to the ring:
  </para>
  <para>
    See ring-preparing
  </para>
  <para>
    See what devices for a server are in the ring:
  </para>
  <programlisting>
swift-ring-builder &lt;builder-file&gt; search &lt;ip_address&gt;
</programlisting>
  <para>
    Once you are done with all changes to the ring, the changes need to
    be &quot;committed&quot;:
  </para>
  <programlisting>
swift-ring-builder &lt;builder-file&gt; rebalance
</programlisting>
  <para>
    Once the new rings are built, they should be pushed out to all the
    servers in the cluster.
  </para>
  <para>
    Optionally, if invoked as 'swift-ring-builder-safe' the directory
    containing the specified builder file will be locked (via a .lock
    file in the parent directory). This provides a basic safe guard
    against multiple instances of the swift-ring-builder (or other
    utilities that observe this lock) from attempting to write to or
    read the builder/ring files while operations are in progress. This
    can be useful in environments where ring management has been
    automated but the operator still needs to interact with the rings
    manually.
  </para>
</sect1>
</section>
<section xml:id="scripting-ring-creation">
  <title>Scripting Ring Creation</title>
  <para>
    You can create scripts to create the account and container rings and
    rebalance. Here's an example script for the Account ring. Use
    similar commands to create a make-container-ring.sh script on the
    proxy server node.
  </para>
  <orderedlist numeration="arabic">
    <listitem>
      <para>
        Create a script file called make-account-ring.sh on the proxy
        server node with the following content:
      </para>
      <programlisting>
#!/bin/bash
cd /etc/swift
rm -f account.builder account.ring.gz backups/account.builder backups/account.ring.gz
swift-ring-builder account.builder create 18 3 1
swift-ring-builder account.builder add z1-&lt;account-server-1&gt;:6002/sdb1 1
swift-ring-builder account.builder add z2-&lt;account-server-2&gt;:6002/sdb1 1
swift-ring-builder account.builder rebalance
</programlisting>
      <para>
        You need to replace the values of &lt;account-server-1&gt;,
        &lt;account-server-2&gt;, etc. with the IP addresses of the
        account servers used in your setup. You can have as many account
        servers as you need. All account servers are assumed to be
        listening on port 6002, and have a storage device called
        &quot;sdb1&quot; (this is a directory name created under /drives
        when we setup the account server). The &quot;z1&quot;,
        &quot;z2&quot;, etc. designate zones, and you can choose whether
        you put devices in the same or different zones.
      </para>
    </listitem>
    <listitem>
      <para>
        Make the script file executable and run it to create the account
        ring file:
      </para>
      <programlisting>
chmod +x make-account-ring.sh
sudo ./make-account-ring.sh
</programlisting>
    </listitem>
    <listitem>
      <para>
        Copy the resulting ring file /etc/swift/account.ring.gz to all
        the account server nodes in your Swift environment, and put them
        in the /etc/swift directory on these nodes. Make sure that every
        time you change the account ring configuration, you copy the
        resulting ring file to all the account nodes.
      </para>
    </listitem>
  </orderedlist>
</section>
<section xml:id="handling-system-updates">
  <title>Handling System Updates</title>
  <para>
    It is recommended that system updates and reboots are done a zone at
    a time. This allows the update to happen, and for the Swift cluster
    to stay available and responsive to requests. It is also advisable
    when updating a zone, let it run for a while before updating the
    other zones to make sure the update doesn't have any adverse
    effects.
  </para>
</section>
<section xml:id="handling-drive-failure">
  <title>Handling Drive Failure</title>
  <para>
    In the event that a drive has failed, the first step is to make sure
    the drive is unmounted. This will make it easier for swift to work
    around the failure until it has been resolved. If the drive is going
    to be replaced immediately, then it is just best to replace the
    drive, format it, remount it, and let replication fill it up.
  </para>
  <para>
    If the drive can't be replaced immediately, then it is best to leave
    it unmounted, and remove the drive from the ring. This will allow
    all the replicas that were on that drive to be replicated elsewhere
    until the drive is replaced. Once the drive is replaced, it can be
    re-added to the ring.
  </para>
</section>
<section xml:id="handling-server-failure">
  <title>Handling Server Failure</title>
  <para>
    If a server is having hardware issues, it is a good idea to make
    sure the swift services are not running. This will allow Swift to
    work around the failure while you troubleshoot.
  </para>
  <para>
    If the server just needs a reboot, or a small amount of work that
    should only last a couple of hours, then it is probably best to let
    Swift work around the failure and get the machine fixed and back
    online. When the machine comes back online, replication will make
    sure that anything that is missing during the downtime will get
    updated.
  </para>
  <para>
    If the server has more serious issues, then it is probably best to
    remove all of the server's devices from the ring. Once the server
    has been repaired and is back online, the server's devices can be
    added back into the ring. It is important that the devices are
    reformatted before putting them back into the ring as it is likely
    to be responsible for a different set of partitions than before.
  </para>
</section>
<section xml:id="detecting-failed-drives">
  <title>Detecting Failed Drives</title>
  <para>
    It has been our experience that when a drive is about to fail, error
    messages will spew into /var/log/kern.log. There is a script called
    swift-drive-audit that can be run via cron to watch for bad drives.
    If errors are detected, it will unmount the bad drive, so that Swift
    can work around it. The script takes a configuration file with the
    following settings:
  </para>
  <para>
    [drive-audit]
  </para>
  <informaltable>
    <tgroup cols="3">
      <colspec align="left" />
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Option
          </entry>
          <entry>
            Default
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            log_facility
          </entry>
          <entry>
            LOG_LOCAL0
          </entry>
          <entry>
            Syslog log facility
          </entry>
        </row>
        <row>
          <entry>
            log_level
          </entry>
          <entry>
            INFO
          </entry>
          <entry>
            Log level
          </entry>
        </row>
        <row>
          <entry>
            device_dir
          </entry>
          <entry>
            /srv/node
          </entry>
          <entry>
            Directory devices are mounted under
          </entry>
        </row>
        <row>
          <entry>
            minutes
          </entry>
          <entry>
            60
          </entry>
          <entry>
            Number of minutes to look back in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
          </entry>
          <entry>
            /var/log/kern.log
          </entry>
        </row>
        <row>
          <entry>
            error_limit
          </entry>
          <entry>
            1
          </entry>
          <entry>
            Number of errors to find before a device
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
          </entry>
          <entry>
            is unmounted
          </entry>
        </row>
        <row>
          <entry>
            log_file_pattern
          </entry>
          <entry>
            /var/log/kern*
          </entry>
          <entry>
            Location of the log file with globbing
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
          </entry>
          <entry>
            pattern to check against device errors
          </entry>
        </row>
        <row>
          <entry>
            regex_pattern_X
          </entry>
          <entry>
            (see below)
          </entry>
          <entry>
            Regular expression patterns to be used to
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
          </entry>
          <entry>
            locate device blocks with errors in the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
          </entry>
          <entry>
            log file
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    The default regex pattern used to locate device blocks with errors
    are \berror\b.*\b(sd[a-z]{1,2}\d?)\b and
    \b(sd[a-z]{1,2}\d?)\b.*\berror\b. One is able to overwrite the
    default above by providing new expressions using the format
    regex_pattern_X = regex_expression, where X is a number.
  </para>
  <para>
    This script has been tested on Ubuntu 10.04 and Ubuntu 12.04, so if
    you are using a different distro or OS, some care should be taken
    before using in production.
  </para>
</section>
<section xml:id="cluster-health">
  <title>Cluster Health</title>
  <para>
    There is a swift-dispersion-report tool for measuring overall
    cluster health. This is accomplished by checking if a set of
    deliberately distributed containers and objects are currently in
    their proper places within the cluster.
  </para>
  <para>
    For instance, a common deployment has three replicas of each object.
    The health of that object can be measured by checking if each
    replica is in its proper place. If only 2 of the 3 is in place the
    object's heath can be said to be at 66.66%, where 100% would be
    perfect.
  </para>
  <para>
    A single object's health, especially an older object, usually
    reflects the health of that entire partition the object is in. If we
    make enough objects on a distinct percentage of the partitions in
    the cluster, we can get a pretty valid estimate of the overall
    cluster health. In practice, about 1% partition coverage seems to
    balance well between accuracy and the amount of time it takes to
    gather results.
  </para>
  <para>
    The first thing that needs to be done to provide this health value
    is create a new account solely for this usage. Next, we need to
    place the containers and objects throughout the system so that they
    are on distinct partitions. The swift-dispersion-populate tool does
    this by making up random container and object names until they fall
    on distinct partitions. Last, and repeatedly for the life of the
    cluster, we need to run the swift-dispersion-report tool to check
    the health of each of these containers and objects.
  </para>
  <para>
    These tools need direct access to the entire cluster and to the ring
    files (installing them on a proxy server will probably do). Both
    swift-dispersion-populate and swift-dispersion-report use the same
    configuration file, /etc/swift/dispersion.conf. Example conf file:
  </para>
  <programlisting>
[dispersion]
auth_url = http://localhost:8080/auth/v1.0
auth_user = test:tester
auth_key = testing
endpoint_type = internalURL
</programlisting>
  <para>
    There are also options for the conf file for specifying the
    dispersion coverage (defaults to 1%), retries, concurrency, etc.
    though usually the defaults are fine.
  </para>
  <para>
    Once the configuration is in place, run swift-dispersion-populate to
    populate the containers and objects throughout the cluster.
  </para>
  <para>
    Now that those containers and objects are in place, you can run
    swift-dispersion-report to get a dispersion report, or the overall
    health of the cluster. Here is an example of a cluster in perfect
    health:
  </para>
  <programlisting>
$ swift-dispersion-report
Queried 2621 containers for dispersion reporting, 19s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

Queried 2619 objects for dispersion reporting, 7s, 0 retries
100.00% of object copies found (7857 of 7857)
Sample represents 1.00% of the object partition space
</programlisting>
  <para>
    Now I'll deliberately double the weight of a device in the object
    ring (with replication turned off) and rerun the dispersion report
    to show what impact that has:
  </para>
  <programlisting>
$ swift-ring-builder object.builder set_weight d0 200
$ swift-ring-builder object.builder rebalance
...
$ swift-dispersion-report
Queried 2621 containers for dispersion reporting, 8s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

Queried 2619 objects for dispersion reporting, 7s, 0 retries
There were 1763 partitions missing one copy.
77.56% of object copies found (6094 of 7857)
Sample represents 1.00% of the object partition space
</programlisting>
  <para>
    You can see the health of the objects in the cluster has gone down
    significantly. Of course, I only have four devices in this test
    environment, in a production environment with many many devices the
    impact of one device change is much less. Next, I'll run the
    replicators to get everything put back into place and then rerun the
    dispersion report:
  </para>
  <programlisting>
... start object replicators and monitor logs until they're caught up ...
$ swift-dispersion-report
Queried 2621 containers for dispersion reporting, 17s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

Queried 2619 objects for dispersion reporting, 7s, 0 retries
100.00% of object copies found (7857 of 7857)
Sample represents 1.00% of the object partition space
</programlisting>
  <para>
    You can also run the report for only containers or objects:
  </para>
  <programlisting>
$ swift-dispersion-report --container-only
Queried 2621 containers for dispersion reporting, 17s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

$ swift-dispersion-report --object-only
Queried 2619 objects for dispersion reporting, 7s, 0 retries
100.00% of object copies found (7857 of 7857)
Sample represents 1.00% of the object partition space
</programlisting>
  <para>
    Alternatively, the dispersion report can also be output in json
    format. This allows it to be more easily consumed by third party
    utilities:
  </para>
  <programlisting>
$ swift-dispersion-report -j
{&quot;object&quot;: {&quot;retries:&quot;: 0, &quot;missing_two&quot;: 0, &quot;copies_found&quot;: 7863, &quot;missing_one&quot;: 0, &quot;copies_expected&quot;: 7863, &quot;pct_found&quot;: 100.0, &quot;overlapping&quot;: 0, &quot;missing_all&quot;: 0}, &quot;container&quot;: {&quot;retries:&quot;: 0, &quot;missing_two&quot;: 0, &quot;copies_found&quot;: 12534, &quot;missing_one&quot;: 0, &quot;copies_expected&quot;: 12534, &quot;pct_found&quot;: 100.0, &quot;overlapping&quot;: 15, &quot;missing_all&quot;: 0}}
</programlisting>
</section>
<section xml:id="geographically-distributed-clusters">
  <title>Geographically Distributed Clusters</title>
  <para>
    Swift's default configuration is currently designed to work in a
    single region, where a region is defined as a group of machines with
    high-bandwidth, low-latency links between them. However,
    configuration options exist that make running a performant
    multi-region Swift cluster possible.
  </para>
  <para>
    For the rest of this section, we will assume a two-region Swift
    cluster: region 1 in San Francisco (SF), and region 2 in New York
    (NY). Each region shall contain within it 3 zones, numbered 1, 2,
    and 3, for a total of 6 zones.
  </para>
<section xml:id="read_affinity">
    <title>read_affinity</title>
    <para>
      This setting makes the proxy server prefer local backend servers
      for GET and HEAD requests over non-local ones. For example, it is
      preferable for an SF proxy server to service object GET requests
      by talking to SF object servers, as the client will receive lower
      latency and higher throughput.
    </para>
    <para>
      By default, Swift randomly chooses one of the three replicas to
      give to the client, thereby spreading the load evenly. In the case
      of a geographically-distributed cluster, the administrator is
      likely to prioritize keeping traffic local over even distribution
      of results. This is where the read_affinity setting comes in.
    </para>
    <para>
      Example:
    </para>
    <programlisting>
[app:proxy-server]
read_affinity = r1=100
</programlisting>
    <para>
      This will make the proxy attempt to service GET and HEAD requests
      from backends in region 1 before contacting any backends in region
      2. However, if no region 1 backends are available (due to replica
      placement, failed hardware, or other reasons), then the proxy will
      fall back to backend servers in other regions.
    </para>
    <para>
      Example:
    </para>
    <programlisting>
[app:proxy-server]
read_affinity = r1z1=100, r1=200
</programlisting>
    <para>
      This will make the proxy attempt to service GET and HEAD requests
      from backends in region 1 zone 1, then backends in region 1, then
      any other backends. If a proxy is physically close to a particular
      zone or zones, this can provide bandwidth savings. For example, if
      a zone corresponds to servers in a particular rack, and the proxy
      server is in that same rack, then setting read_affinity to prefer
      reads from within the rack will result in less traffic between the
      top-of-rack switches.
    </para>
    <para>
      The read_affinity setting may contain any number of region/zone
      specifiers; the priority number (after the equals sign) determines
      the ordering in which backend servers will be contacted. A lower
      number means higher priority.
    </para>
    <para>
      Note that read_affinity only affects the ordering of primary nodes
      (see ring docs for definition of primary node), not the ordering
      of handoff nodes.
    </para>
</section>
<section xml:id="write_affinity-and-write_affinity_node_count">
    <title>write_affinity and write_affinity_node_count</title>
    <para>
      This setting makes the proxy server prefer local backend servers
      for object PUT requests over non-local ones. For example, it may
      be preferable for an SF proxy server to service object PUT
      requests by talking to SF object servers, as the client will
      receive lower latency and higher throughput. However, if this
      setting is used, note that a NY proxy server handling a GET
      request for an object that was PUT using write affinity may have
      to fetch it across the WAN link, as the object won't immediately
      have any replicas in NY. However, replication will move the
      object's replicas to their proper homes in both SF and NY.
    </para>
    <para>
      Note that only object PUT requests are affected by the
      write_affinity setting; POST, GET, HEAD, DELETE, OPTIONS, and
      account/container PUT requests are not affected.
    </para>
    <para>
      This setting lets you trade data distribution for throughput. If
      write_affinity is enabled, then object replicas will initially be
      stored all within a particular region or zone, thereby decreasing
      the quality of the data distribution, but the replicas will be
      distributed over fast WAN links, giving higher throughput to
      clients. Note that the replicators will eventually move objects to
      their proper, well-distributed homes.
    </para>
    <para>
      The write_affinity setting is useful only when you don't typically
      read objects immediately after writing them. For example, consider
      a workload of mainly backups: if you have a bunch of machines in
      NY that periodically write backups to Swift, then odds are that
      you don't then immediately read those backups in SF. If your
      workload doesn't look like that, then you probably shouldn't use
      write_affinity.
    </para>
    <para>
      The write_affinity_node_count setting is only useful in
      conjunction with write_affinity; it governs how many local object
      servers will be tried before falling back to non-local ones.
    </para>
    <para>
      Example:
    </para>
    <programlisting>
[app:proxy-server]
write_affinity = r1
write_affinity_node_count = 2 * replicas
</programlisting>
    <para>
      Assuming 3 replicas, this configuration will make object PUTs try
      storing the object's replicas on up to 6 disks (&quot;2 *
      replicas&quot;) in region 1 (&quot;r1&quot;).
    </para>
    <para>
      You should be aware that, if you have data coming into SF faster
      than your link to NY can transfer it, then your cluster's data
      distribution will get worse and worse over time as objects pile up
      in SF. If this happens, it is recommended to disable
      write_affinity and simply let object PUTs traverse the WAN link,
      as that will naturally limit the object growth rate to what your
      WAN link can handle.
    </para>
</section>
</section>
<section xml:id="cluster-telemetry-and-monitoring">
  <title>Cluster Telemetry and Monitoring</title>
  <para>
    Various metrics and telemetry can be obtained from the account,
    container, and object servers using the recon server middleware and
    the swift-recon cli. To do so update your account, container, or
    object servers pipelines to include recon and add the associated
    filter config.
  </para>
  <para>
    object-server.conf sample:
  </para>
  <programlisting>
[pipeline:main]
pipeline = recon object-server

[filter:recon]
use = egg:swift#recon
recon_cache_path = /var/cache/swift
</programlisting>
  <para>
    container-server.conf sample:
  </para>
  <programlisting>
[pipeline:main]
pipeline = recon container-server

[filter:recon]
use = egg:swift#recon
recon_cache_path = /var/cache/swift
</programlisting>
  <para>
    account-server.conf sample:
  </para>
  <programlisting>
[pipeline:main]
pipeline = recon account-server

[filter:recon]
use = egg:swift#recon
recon_cache_path = /var/cache/swift
</programlisting>
  <para>
    The recon_cache_path simply sets the directory where stats for a few
    items will be stored. Depending on the method of deployment you may
    need to create this directory manually and ensure that swift has
    read/write access.
  </para>
  <para>
    Finally, if you also wish to track asynchronous pending on your
    object servers you will need to setup a cronjob to run the
    swift-recon-cron script periodically on your object servers:
  </para>
  <programlisting>
*/5 * * * * swift /usr/bin/swift-recon-cron /etc/swift/object-server.conf
</programlisting>
  <para>
    Once the recon middleware is enabled, a GET request for
    &quot;/recon/&lt;metric&gt;&quot; to the backend object server will
    return a JSON-formatted response:
  </para>
  <programlisting>
fhines@ubuntu:~$ curl -i http://localhost:6030/recon/async
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 20
Date: Tue, 18 Oct 2011 21:03:01 GMT

{&quot;async_pending&quot;: 0}
</programlisting>
  <para>
    Note that the default port for the object server is 6000, except on
    a Swift All-In-One installation, which uses 6010, 6020, 6030, and
    6040.
  </para>
  <para>
    The following metrics and telemetry are currently exposed:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Request URI
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            /recon/load
          </entry>
          <entry>
            returns 1,5, and 15 minute load average
          </entry>
        </row>
        <row>
          <entry>
            /recon/mem
          </entry>
          <entry>
            returns /proc/meminfo
          </entry>
        </row>
        <row>
          <entry>
            /recon/mounted
          </entry>
          <entry>
            returns <emphasis>ALL</emphasis> currently mounted
            filesystems
          </entry>
        </row>
        <row>
          <entry>
            /recon/unmounted
          </entry>
          <entry>
            returns all unmounted drives if mount_check = True
          </entry>
        </row>
        <row>
          <entry>
            /recon/diskusage
          </entry>
          <entry>
            returns disk utilization for storage devices
          </entry>
        </row>
        <row>
          <entry>
            /recon/ringmd5
          </entry>
          <entry>
            returns object/container/account ring md5sums
          </entry>
        </row>
        <row>
          <entry>
            /recon/quarantined
          </entry>
          <entry>
            returns # of quarantined objects/accounts/containers
          </entry>
        </row>
        <row>
          <entry>
            /recon/sockstat
          </entry>
          <entry>
            returns consumable info from /proc/net/sockstat|6
          </entry>
        </row>
        <row>
          <entry>
            /recon/devices
          </entry>
          <entry>
            returns list of devices and devices dir i.e. /srv/node
          </entry>
        </row>
        <row>
          <entry>
            /recon/async
          </entry>
          <entry>
            returns count of async pending
          </entry>
        </row>
        <row>
          <entry>
            /recon/replication
          </entry>
          <entry>
            returns object replication times (for backward
            compatibility)
          </entry>
        </row>
        <row>
          <entry>
            /recon/replication/&lt;type&gt;
          </entry>
          <entry>
            returns replication info for given type (account, container,
            object)
          </entry>
        </row>
        <row>
          <entry>
            /recon/auditor/&lt;type&gt;
          </entry>
          <entry>
            returns auditor stats on last reported scan for given type
            (account, container, object)
          </entry>
        </row>
        <row>
          <entry>
            /recon/updater/&lt;type&gt;
          </entry>
          <entry>
            returns last updater sweep times for given type (container,
            object)
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    This information can also be queried via the swift-recon command
    line utility:
  </para>
  <programlisting>
fhines@ubuntu:~$ swift-recon -h
Usage: 
        usage: swift-recon &lt;server_type&gt; [-v] [--suppress] [-a] [-r] [-u] [-d]
        [-l] [--md5] [--auditor] [--updater] [--expirer] [--sockstat]

        &lt;server_type&gt;   account|container|object
        Defaults to object server.

        ex: swift-recon container -l --auditor


Options:
  -h, --help            show this help message and exit
  -v, --verbose         Print verbose info
  --suppress            Suppress most connection related errors
  -a, --async           Get async stats
  -r, --replication     Get replication stats
  --auditor             Get auditor stats
  --updater             Get updater stats
  --expirer             Get expirer stats
  -u, --unmounted       Check cluster for unmounted devices
  -d, --diskusage       Get disk usage stats
  -l, --loadstats       Get cluster load average stats
  -q, --quarantined     Get cluster quarantine stats
  --md5                 Get md5sum of servers ring and compare to local copy
  --sockstat            Get cluster socket usage stats
  --all                 Perform all checks. Equal to -arudlq --md5 --sockstat
  -z ZONE, --zone=ZONE  Only query servers in specified zone
  -t SECONDS, --timeout=SECONDS
                        Time to wait for a response from a server
  --swiftdir=SWIFTDIR   Default = /etc/swift
</programlisting>
  <para>
    For example, to obtain container replication info from all hosts in
    zone &quot;3&quot;:
  </para>
  <programlisting>
fhines@ubuntu:~$ swift-recon container -r --zone 3
===============================================================================
--&gt; Starting reconnaissance on 1 hosts
===============================================================================
[2012-04-02 02:45:48] Checking on replication
[failure] low: 0.000, high: 0.000, avg: 0.000, reported: 1
[success] low: 486.000, high: 486.000, avg: 486.000, reported: 1
[replication_time] low: 20.853, high: 20.853, avg: 20.853, reported: 1
[attempted] low: 243.000, high: 243.000, avg: 243.000, reported: 1
</programlisting>
</section>
<section xml:id="reporting-metrics-to-statsd">
  <title>Reporting Metrics to StatsD</title>
  <para>
    If you have a
    <link xlink:href="http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/">StatsD</link>
    server running, Swift may be configured to send it real-time
    operational metrics. To enable this, set the following configuration
    entries (see the sample configuration files):
  </para>
  <programlisting>
log_statsd_host = localhost
log_statsd_port = 8125
log_statsd_default_sample_rate = 1.0
log_statsd_sample_rate_factor = 1.0
log_statsd_metric_prefix =                [empty-string]
</programlisting>
  <para>
    If log_statsd_host is not set, this feature is disabled. The default
    values for the other settings are given above.
  </para>
  <para>
    The sample rate is a real number between 0 and 1 which defines the
    probability of sending a sample for any given event or timing
    measurement. This sample rate is sent with each sample to StatsD and
    used to multiply the value. For example, with a sample rate of 0.5,
    StatsD will multiply that counter's value by 2 when flushing the
    metric to an upstream monitoring system
    (<link xlink:href="http://graphite.wikidot.com/">Graphite</link>,
    <link xlink:href="http://ganglia.sourceforge.net/">Ganglia</link>, etc.).
  </para>
  <para>
    Some relatively high-frequency metrics have a default sample rate
    less than one. If you want to override the default sample rate for
    all metrics whose default sample rate is not specified in the Swift
    source, you may set log_statsd_default_sample_rate to a value less
    than one. This is NOT recommended (see next paragraph). A better way
    to reduce StatsD load is to adjust log_statsd_sample_rate_factor to
    a value less than one. The log_statsd_sample_rate_factor is
    multiplied to any sample rate (either the global default or one
    specified by the actual metric logging call in the Swift source)
    prior to handling. In other words, this one tunable can lower the
    frequency of all StatsD logging by a proportional amount.
  </para>
  <para>
    To get the best data, start with the default
    log_statsd_default_sample_rate and log_statsd_sample_rate_factor
    values of 1 and only lower log_statsd_sample_rate_factor if needed.
    The log_statsd_default_sample_rate should not be used and remains
    for backward compatibility only.
  </para>
  <para>
    The metric prefix will be prepended to every metric sent to the
    StatsD server For example, with:
  </para>
  <programlisting>
log_statsd_metric_prefix = proxy01
</programlisting>
  <para>
    the metric proxy-server.errors would be sent to StatsD as
    proxy01.proxy-server.errors. This is useful for differentiating
    different servers when sending statistics to a central StatsD
    server. If you run a local StatsD server per node, you could
    configure a per-node metrics prefix there and leave
    log_statsd_metric_prefix blank.
  </para>
  <para>
    Note that metrics reported to StatsD are counters or timing data
    (which are sent in units of milliseconds). StatsD usually expands
    timing data out to min, max, avg, count, and 90th percentile per
    timing metric, but the details of this behavior will depend on the
    configuration of your StatsD server. Some important
    &quot;gauge&quot; metrics may still need to be collected using
    another method. For example, the object-server.async_pendings StatsD
    metric counts the generation of async_pendings in real-time, but
    will not tell you the current number of async_pending container
    updates on disk at any point in time.
  </para>
  <para>
    Note also that the set of metrics collected, their names, and their
    semantics are not locked down and will change over time. StatsD
    logging is currently in a &quot;beta&quot; stage and will continue
    to evolve.
  </para>
  <para>
    Metrics for `account-auditor`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            account-auditor.errors
          </entry>
          <entry>
            Count of audit runs (across all account databases) which
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            caught an Exception.
          </entry>
        </row>
        <row>
          <entry>
            account-auditor.passes
          </entry>
          <entry>
            Count of individual account databases which passed audit.
          </entry>
        </row>
        <row>
          <entry>
            account-auditor.failures
          </entry>
          <entry>
            Count of individual account databases which failed audit.
          </entry>
        </row>
        <row>
          <entry>
            account-auditor.timing
          </entry>
          <entry>
            Timing data for individual account database audits.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `account-reaper`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            account-reaper.errors
          </entry>
          <entry>
            Count of devices failing the mount check.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.timing
          </entry>
          <entry>
            Timing data for each reap_account() call.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.return_codes.X
          </entry>
          <entry>
            Count of HTTP return codes from various operations
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            (e.g. object listing, container deletion, etc.). The
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            value for X is the first digit of the return code
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            (2 for 201, 4 for 404, etc.).
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.containers_failures
          </entry>
          <entry>
            Count of failures to delete a container.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.containers_deleted
          </entry>
          <entry>
            Count of containers successfully deleted.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.containers_remaining
          </entry>
          <entry>
            Count of containers which failed to delete with
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            zero successes.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.containers_possibly_remaining
          </entry>
          <entry>
            Count of containers which failed to delete with
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            at least one success.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.objects_failures
          </entry>
          <entry>
            Count of failures to delete an object.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.objects_deleted
          </entry>
          <entry>
            Count of objects successfully deleted.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.objects_remaining
          </entry>
          <entry>
            Count of objects which failed to delete with zero
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            successes.
          </entry>
        </row>
        <row>
          <entry>
            account-reaper.objects_possibly_remaining
          </entry>
          <entry>
            Count of objects which failed to delete with at
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            least one success.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for account-server (&quot;Not Found&quot; is not considered
    an error and requests which increment errors are not included in the
    timing data):
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            account-server.DELETE.errors.timing
          </entry>
          <entry>
            Timing data for each DELETE request resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error: bad request, not mounted, missing timestamp.
          </entry>
        </row>
        <row>
          <entry>
            account-server.DELETE.timing
          </entry>
          <entry>
            Timing data for each DELETE request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            account-server.PUT.errors.timing
          </entry>
          <entry>
            Timing data for each PUT request resulting in an error:
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            bad request, not mounted, conflict, recently-deleted.
          </entry>
        </row>
        <row>
          <entry>
            account-server.PUT.timing
          </entry>
          <entry>
            Timing data for each PUT request not resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error.
          </entry>
        </row>
        <row>
          <entry>
            account-server.HEAD.errors.timing
          </entry>
          <entry>
            Timing data for each HEAD request resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error: bad request, not mounted.
          </entry>
        </row>
        <row>
          <entry>
            account-server.HEAD.timing
          </entry>
          <entry>
            Timing data for each HEAD request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            account-server.GET.errors.timing
          </entry>
          <entry>
            Timing data for each GET request resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error: bad request, not mounted, bad delimiter,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            account listing limit too high, bad accept header.
          </entry>
        </row>
        <row>
          <entry>
            account-server.GET.timing
          </entry>
          <entry>
            Timing data for each GET request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            account-server.REPLICATE.errors.timing
          </entry>
          <entry>
            Timing data for each REPLICATE request resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error: bad request, not mounted.
          </entry>
        </row>
        <row>
          <entry>
            account-server.REPLICATE.timing
          </entry>
          <entry>
            Timing data for each REPLICATE request not resulting
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            in an error.
          </entry>
        </row>
        <row>
          <entry>
            account-server.POST.errors.timing
          </entry>
          <entry>
            Timing data for each POST request resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error: bad request, bad or missing timestamp, not
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            mounted.
          </entry>
        </row>
        <row>
          <entry>
            account-server.POST.timing
          </entry>
          <entry>
            Timing data for each POST request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `account-replicator`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            account-replicator.diffs
          </entry>
          <entry>
            Count of syncs handled by sending differing rows.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.diff_caps
          </entry>
          <entry>
            Count of &quot;diffs&quot; operations which failed because
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            &quot;max_diffs&quot; was hit.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.no_changes
          </entry>
          <entry>
            Count of accounts found to be in sync.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.hashmatches
          </entry>
          <entry>
            Count of accounts found to be in sync via hash
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            comparison (broker.merge_syncs was called).
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.rsyncs
          </entry>
          <entry>
            Count of completely missing accounts which were sent
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            via rsync.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.remote_merges
          </entry>
          <entry>
            Count of syncs handled by sending entire database
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            via rsync.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.attempts
          </entry>
          <entry>
            Count of database replication attempts.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.failures
          </entry>
          <entry>
            Count of database replication attempts which failed
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            due to corruption (quarantined) or inability to read
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            as well as attempts to individual nodes which
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            failed.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.removes.&lt;device&gt;
          </entry>
          <entry>
            Count of databases on &lt;device&gt; deleted because the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            delete_timestamp was greater than the put_timestamp
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            and the database had no rows or because it was
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            successfully sync'ed to other locations and doesn't
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            belong here anymore.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.successes
          </entry>
          <entry>
            Count of replication attempts to an individual node
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            which were successful.
          </entry>
        </row>
        <row>
          <entry>
            account-replicator.timing
          </entry>
          <entry>
            Timing data for each database replication attempt
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not resulting in a failure.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `container-auditor`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            container-auditor.errors
          </entry>
          <entry>
            Incremented when an Exception is caught in an audit
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            pass (only once per pass, max).
          </entry>
        </row>
        <row>
          <entry>
            container-auditor.passes
          </entry>
          <entry>
            Count of individual containers passing an audit.
          </entry>
        </row>
        <row>
          <entry>
            container-auditor.failures
          </entry>
          <entry>
            Count of individual containers failing an audit.
          </entry>
        </row>
        <row>
          <entry>
            container-auditor.timing
          </entry>
          <entry>
            Timing data for each container audit.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `container-replicator`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            container-replicator.diffs
          </entry>
          <entry>
            Count of syncs handled by sending differing rows.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.diff_caps
          </entry>
          <entry>
            Count of &quot;diffs&quot; operations which failed because
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            &quot;max_diffs&quot; was hit.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.no_changes
          </entry>
          <entry>
            Count of containers found to be in sync.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.hashmatches
          </entry>
          <entry>
            Count of containers found to be in sync via hash
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            comparison (broker.merge_syncs was called).
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.rsyncs
          </entry>
          <entry>
            Count of completely missing containers where were sent
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            via rsync.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.remote_merges
          </entry>
          <entry>
            Count of syncs handled by sending entire database
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            via rsync.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.attempts
          </entry>
          <entry>
            Count of database replication attempts.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.failures
          </entry>
          <entry>
            Count of database replication attempts which failed
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            due to corruption (quarantined) or inability to read
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            as well as attempts to individual nodes which
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            failed.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.removes.&lt;device&gt;
          </entry>
          <entry>
            Count of databases deleted on &lt;device&gt; because the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            delete_timestamp was greater than the put_timestamp
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            and the database had no rows or because it was
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            successfully sync'ed to other locations and doesn't
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            belong here anymore.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.successes
          </entry>
          <entry>
            Count of replication attempts to an individual node
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            which were successful.
          </entry>
        </row>
        <row>
          <entry>
            container-replicator.timing
          </entry>
          <entry>
            Timing data for each database replication attempt
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not resulting in a failure.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for container-server (&quot;Not Found&quot; is not
    considered an error and requests which increment errors are not
    included in the timing data):
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            container-server.DELETE.errors.timing
          </entry>
          <entry>
            Timing data for DELETE request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not mounted, missing timestamp, conflict.
          </entry>
        </row>
        <row>
          <entry>
            container-server.DELETE.timing
          </entry>
          <entry>
            Timing data for each DELETE request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            container-server.PUT.errors.timing
          </entry>
          <entry>
            Timing data for PUT request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            missing timestamp, not mounted, conflict.
          </entry>
        </row>
        <row>
          <entry>
            container-server.PUT.timing
          </entry>
          <entry>
            Timing data for each PUT request not resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error.
          </entry>
        </row>
        <row>
          <entry>
            container-server.HEAD.errors.timing
          </entry>
          <entry>
            Timing data for HEAD request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not mounted.
          </entry>
        </row>
        <row>
          <entry>
            container-server.HEAD.timing
          </entry>
          <entry>
            Timing data for each HEAD request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            container-server.GET.errors.timing
          </entry>
          <entry>
            Timing data for GET request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not mounted, parameters not utf8, bad accept header.
          </entry>
        </row>
        <row>
          <entry>
            container-server.GET.timing
          </entry>
          <entry>
            Timing data for each GET request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            container-server.REPLICATE.errors.timing
          </entry>
          <entry>
            Timing data for REPLICATE request errors: bad
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            request, not mounted.
          </entry>
        </row>
        <row>
          <entry>
            container-server.REPLICATE.timing
          </entry>
          <entry>
            Timing data for each REPLICATE request not resulting
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            in an error.
          </entry>
        </row>
        <row>
          <entry>
            container-server.POST.errors.timing
          </entry>
          <entry>
            Timing data for POST request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            bad x-container-sync-to, not mounted.
          </entry>
        </row>
        <row>
          <entry>
            container-server.POST.timing
          </entry>
          <entry>
            Timing data for each POST request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `container-sync`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            container-sync.skips
          </entry>
          <entry>
            Count of containers skipped because they don't have
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            sync'ing enabled.
          </entry>
        </row>
        <row>
          <entry>
            container-sync.failures
          </entry>
          <entry>
            Count of failures sync'ing of individual containers.
          </entry>
        </row>
        <row>
          <entry>
            container-sync.syncs
          </entry>
          <entry>
            Count of individual containers sync'ed successfully.
          </entry>
        </row>
        <row>
          <entry>
            container-sync.deletes
          </entry>
          <entry>
            Count of container database rows sync'ed by
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            deletion.
          </entry>
        </row>
        <row>
          <entry>
            container-sync.deletes.timing
          </entry>
          <entry>
            Timing data for each container database row
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            sychronization via deletion.
          </entry>
        </row>
        <row>
          <entry>
            container-sync.puts
          </entry>
          <entry>
            Count of container database rows sync'ed by PUTing.
          </entry>
        </row>
        <row>
          <entry>
            container-sync.puts.timing
          </entry>
          <entry>
            Timing data for each container database row
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            synchronization via PUTing.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `container-updater`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            container-updater.successes
          </entry>
          <entry>
            Count of containers which successfully updated their
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            account.
          </entry>
        </row>
        <row>
          <entry>
            container-updater.failures
          </entry>
          <entry>
            Count of containers which failed to update their
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            account.
          </entry>
        </row>
        <row>
          <entry>
            container-updater.no_changes
          </entry>
          <entry>
            Count of containers which didn't need to update
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            their account.
          </entry>
        </row>
        <row>
          <entry>
            container-updater.timing
          </entry>
          <entry>
            Timing data for processing a container; only
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            includes timing for containers which needed to
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            update their accounts (i.e. &quot;successes&quot; and
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            &quot;failures&quot; but not &quot;no_changes&quot;).
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `object-auditor`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            object-auditor.quarantines
          </entry>
          <entry>
            Count of objects failing audit and quarantined.
          </entry>
        </row>
        <row>
          <entry>
            object-auditor.errors
          </entry>
          <entry>
            Count of errors encountered while auditing objects.
          </entry>
        </row>
        <row>
          <entry>
            object-auditor.timing
          </entry>
          <entry>
            Timing data for each object audit (does not include
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            any rate-limiting sleep time for
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            max_files_per_second, but does include rate-limiting
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            sleep time for max_bytes_per_second).
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `object-expirer`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            object-expirer.objects
          </entry>
          <entry>
            Count of objects expired.
          </entry>
        </row>
        <row>
          <entry>
            object-expirer.errors
          </entry>
          <entry>
            Count of errors encountered while attempting to
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            expire an object.
          </entry>
        </row>
        <row>
          <entry>
            object-expirer.timing
          </entry>
          <entry>
            Timing data for each object expiration attempt,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            including ones resulting in an error.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `object-replicator`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            object-replicator.partition.delete.count.&lt;device&gt;
          </entry>
          <entry>
            A count of partitions on &lt;device&gt; which were
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            replicated to another node because they didn't
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            belong on this node. This metric is tracked
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            per-device to allow for &quot;quiescence detection&quot; for
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            object replication activity on each device.
          </entry>
        </row>
        <row>
          <entry>
            object-replicator.partition.delete.timing
          </entry>
          <entry>
            Timing data for partitions replicated to another
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            node because they didn't belong on this node. This
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            metric is not tracked per device.
          </entry>
        </row>
        <row>
          <entry>
            object-replicator.partition.update.count.&lt;device&gt;
          </entry>
          <entry>
            A count of partitions on &lt;device&gt; which were
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            replicated to another node, but also belong on this
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            node. As with delete.count, this metric is tracked
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            per-device.
          </entry>
        </row>
        <row>
          <entry>
            object-replicator.partition.update.timing
          </entry>
          <entry>
            Timing data for partitions replicated which also
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            belong on this node. This metric is not tracked
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            per-device.
          </entry>
        </row>
        <row>
          <entry>
            object-replicator.suffix.hashes
          </entry>
          <entry>
            Count of suffix directories whose hash (of filenames)
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            was recalculated.
          </entry>
        </row>
        <row>
          <entry>
            object-replicator.suffix.syncs
          </entry>
          <entry>
            Count of suffix directories replicated with rsync.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `object-server`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            object-server.quarantines
          </entry>
          <entry>
            Count of objects (files) found bad and moved to
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            quarantine.
          </entry>
        </row>
        <row>
          <entry>
            object-server.async_pendings
          </entry>
          <entry>
            Count of container updates saved as async_pendings
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            (may result from PUT or DELETE requests).
          </entry>
        </row>
        <row>
          <entry>
            object-server.POST.errors.timing
          </entry>
          <entry>
            Timing data for POST request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            missing timestamp, delete-at in past, not mounted.
          </entry>
        </row>
        <row>
          <entry>
            object-server.POST.timing
          </entry>
          <entry>
            Timing data for each POST request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error.
          </entry>
        </row>
        <row>
          <entry>
            object-server.PUT.errors.timing
          </entry>
          <entry>
            Timing data for PUT request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not mounted, missing timestamp, object creation
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            constraint violation, delete-at in past.
          </entry>
        </row>
        <row>
          <entry>
            object-server.PUT.timeouts
          </entry>
          <entry>
            Count of object PUTs which exceeded max_upload_time.
          </entry>
        </row>
        <row>
          <entry>
            object-server.PUT.timing
          </entry>
          <entry>
            Timing data for each PUT request not resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error.
          </entry>
        </row>
        <row>
          <entry>
            object-server.PUT.&lt;device&gt;.timing
          </entry>
          <entry>
            Timing data per kB transfered (ms/kB) for each
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            non-zero-byte PUT request on each device.
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            Monitoring problematic devices, higher is bad.
          </entry>
        </row>
        <row>
          <entry>
            object-server.GET.errors.timing
          </entry>
          <entry>
            Timing data for GET request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not mounted, header timestamps before the epoch,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            precondition failed.
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            File errors resulting in a quarantine are not
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            counted here.
          </entry>
        </row>
        <row>
          <entry>
            object-server.GET.timing
          </entry>
          <entry>
            Timing data for each GET request not resulting in an
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            error. Includes requests which couldn't find the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            object (including disk errors resulting in file
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            quarantine).
          </entry>
        </row>
        <row>
          <entry>
            object-server.HEAD.errors.timing
          </entry>
          <entry>
            Timing data for HEAD request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            not mounted.
          </entry>
        </row>
        <row>
          <entry>
            object-server.HEAD.timing
          </entry>
          <entry>
            Timing data for each HEAD request not resulting in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            an error. Includes requests which couldn't find the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            object (including disk errors resulting in file
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            quarantine).
          </entry>
        </row>
        <row>
          <entry>
            object-server.DELETE.errors.timing
          </entry>
          <entry>
            Timing data for DELETE request errors: bad request,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            missing timestamp, not mounted, precondition
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            failed. Includes requests which couldn't find or
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            match the object.
          </entry>
        </row>
        <row>
          <entry>
            object-server.DELETE.timing
          </entry>
          <entry>
            Timing data for each DELETE request not resulting
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            in an error.
          </entry>
        </row>
        <row>
          <entry>
            object-server.REPLICATE.errors.timing
          </entry>
          <entry>
            Timing data for REPLICATE request errors: bad
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            request, not mounted.
          </entry>
        </row>
        <row>
          <entry>
            object-server.REPLICATE.timing
          </entry>
          <entry>
            Timing data for each REPLICATE request not resulting
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            in an error.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for `object-updater`:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            object-updater.errors
          </entry>
          <entry>
            Count of drives not mounted or async_pending files
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            with an unexpected name.
          </entry>
        </row>
        <row>
          <entry>
            object-updater.timing
          </entry>
          <entry>
            Timing data for object sweeps to flush async_pending
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            container updates. Does not include object sweeps
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            which did not find an existing async_pending storage
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            directory.
          </entry>
        </row>
        <row>
          <entry>
            object-updater.quarantines
          </entry>
          <entry>
            Count of async_pending container updates which were
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            corrupted and moved to quarantine.
          </entry>
        </row>
        <row>
          <entry>
            object-updater.successes
          </entry>
          <entry>
            Count of successful container updates.
          </entry>
        </row>
        <row>
          <entry>
            object-updater.failures
          </entry>
          <entry>
            Count of failed container updates.
          </entry>
        </row>
        <row>
          <entry>
            object-updater.unlinks
          </entry>
          <entry>
            Count of async_pending files unlinked. An
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            async_pending file is unlinked either when it is
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            successfully processed or when the replicator sees
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            that there is a newer async_pending file for the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            same object.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for proxy-server (in the table, &lt;type&gt; is the
    proxy-server controller responsible for the request and will be one
    of &quot;account&quot;, &quot;container&quot;, or
    &quot;object&quot;):
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            proxy-server.errors
          </entry>
          <entry>
            Count of errors encountered while serving requests
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            before the controller type is determined. Includes
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            invalid Content-Length, errors finding the internal
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            controller to handle the request, invalid utf8, and
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            bad URLs.
          </entry>
        </row>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.handoff_count
          </entry>
          <entry>
            Count of node hand-offs; only tracked if log_handoffs
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            is set in the proxy-server config.
          </entry>
        </row>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.handoff_all_count
          </entry>
          <entry>
            Count of times <emphasis>only</emphasis> hand-off locations
            were
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            utilized; only tracked if log_handoffs is set in the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            proxy-server config.
          </entry>
        </row>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.client_timeouts
          </entry>
          <entry>
            Count of client timeouts (client did not read within
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            client_timeout seconds during a GET or did not
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            supply data within client_timeout seconds during
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            a PUT).
          </entry>
        </row>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.client_disconnects
          </entry>
          <entry>
            Count of detected client disconnects during PUT
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            operations (does NOT include caught Exceptions in
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            the proxy-server which caused a client disconnect).
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for proxy-logging middleware (in the table, &lt;type&gt; is
    either the proxy-server controller responsible for the request:
    &quot;account&quot;, &quot;container&quot;, &quot;object&quot;, or
    the string &quot;SOS&quot; if the request came from the
    <link xlink:href="https://github.com/dpgoetz/sos">Swift Origin
    Server</link> middleware. The &lt;verb&gt; portion will be one of
    &quot;GET&quot;, &quot;HEAD&quot;, &quot;POST&quot;,
    &quot;PUT&quot;, &quot;DELETE&quot;, &quot;COPY&quot;,
    &quot;OPTIONS&quot;, or &quot;BAD_METHOD&quot;. The list of valid
    HTTP methods is configurable via the log_statsd_valid_http_methods
    config variable and the default setting yields the above behavior.
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.&lt;verb&gt;.&lt;status&gt;.timing
          </entry>
          <entry>
            Timing data for requests, start to finish.
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            The &lt;status&gt; portion is the numeric HTTP
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            status code for the request (e.g. &quot;200&quot; or
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            &quot;404&quot;).
          </entry>
        </row>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.GET.&lt;status&gt;.first-byte.timing
          </entry>
          <entry>
            Timing data up to completion of sending the
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            response headers (only for GET requests).
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            &lt;status&gt; and &lt;type&gt; are as for the main
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            timing metric.
          </entry>
        </row>
        <row>
          <entry>
            proxy-server.&lt;type&gt;.&lt;verb&gt;.&lt;status&gt;.xfer
          </entry>
          <entry>
            This counter metric is the sum of bytes
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            transferred in (from clients) and out (to
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            clients) for requests. The &lt;type&gt;, &lt;verb&gt;,
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            and &lt;status&gt; portions of the metric are just
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            like the main timing metric.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    Metrics for tempauth middleware (in the table,
    &lt;reseller_prefix&gt; represents the actual configured
    reseller_prefix or &quot;NONE&quot; if the reseller_prefix is the
    empty string):
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Metric Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            tempauth.&lt;reseller_prefix&gt;.unauthorized
          </entry>
          <entry>
            Count of regular requests which were denied with
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            HTTPUnauthorized.
          </entry>
        </row>
        <row>
          <entry>
            tempauth.&lt;reseller_prefix&gt;.forbidden
          </entry>
          <entry>
            Count of regular requests which were denied with
          </entry>
        </row>
        <row>
          <entry>
          </entry>
          <entry>
            HTTPForbidden.
          </entry>
        </row>
        <row>
          <entry>
            tempauth.&lt;reseller_prefix&gt;.token_denied
          </entry>
          <entry>
            Count of token requests which were denied.
          </entry>
        </row>
        <row>
          <entry>
            tempauth.&lt;reseller_prefix&gt;.errors
          </entry>
          <entry>
            Count of errors.
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</section>
<section xml:id="debugging-tips-and-tools">
  <title>Debugging Tips and Tools</title>
  <para>
    When a request is made to Swift, it is given a unique transaction
    id. This id should be in every log line that has to do with that
    request. This can be useful when looking at all the services that
    are hit by a single request.
  </para>
  <para>
    If you need to know where a specific account, container or object is
    in the cluster, swift-get-nodes will show the location where each
    replica should be.
  </para>
  <para>
    If you are looking at an object on the server and need more info,
    swift-object-info will display the account, container, replica
    locations and metadata of the object.
  </para>
  <para>
    If you want to audit the data for an account, swift-account-audit
    can be used to crawl the account, checking that all containers and
    objects can be found.
  </para>
</section>
<section xml:id="managing-services">
  <title>Managing Services</title>
  <para>
    Swift services are generally managed with swift-init. the general
    usage is
    <literal>swift-init &lt;service&gt; &lt;command&gt;</literal>, where
    service is the swift service to manage (for example object,
    container, account, proxy) and command is one of:
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Command
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            start
          </entry>
          <entry>
            Start the service
          </entry>
        </row>
        <row>
          <entry>
            stop
          </entry>
          <entry>
            Stop the service
          </entry>
        </row>
        <row>
          <entry>
            restart
          </entry>
          <entry>
            Restart the service
          </entry>
        </row>
        <row>
          <entry>
            shutdown
          </entry>
          <entry>
            Attempt to gracefully shutdown the service
          </entry>
        </row>
        <row>
          <entry>
            reload
          </entry>
          <entry>
            Attempt to gracefully restart the service
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    A graceful shutdown or reload will finish any current requests
    before completely stopping the old service. There is also a special
    case of swift-init all &lt;command&gt;, which will run the command
    for all swift services.
  </para>
</section>
<section xml:id="object-auditor">
  <title>Object Auditor</title>
  <para>
    On system failures, the XFS file system can sometimes truncate files
    it's trying to write and produce zero-byte files. The object-auditor
    will catch these problems but in the case of a system crash it would
    be advisable to run an extra, less rate limited sweep to check for
    these specific files. You can run this command as follows:
    swift-object-auditor /path/to/object-server/config/file.conf once -z 1000
    &quot;-z&quot; means to only check for zero-byte files at 1000 files
    per second.
  </para>
</section>
<section xml:id="object-replicator">
  <title>Object Replicator</title>
  <para>
    At times it is useful to be able to run the object replicator on a
    specific device or partition. You can run the object-replicator as
    follows: swift-object-replicator
    /path/to/object-server/config/file.conf once --devices=sda,sdb
  </para>
  <para>
    This will run the object replicator on only the sda and sdb devices.
    You can likewise run that command with --partitions. Both params
    accept a comma separated list of values. If both are specified they
    will be ANDed together. These can only be run in &quot;once&quot;
    mode.
  </para>
</section>
<section xml:id="swift-orphans">
  <title>Swift Orphans</title>
  <para>
    Swift Orphans are processes left over after a reload of a Swift
    server.
  </para>
  <para>
    For example, when upgrading a proxy server you would probaby finish
    with a swift-init proxy-server reload or /etc/init.d/swift-proxy
    reload. This kills the parent proxy server process and leaves the
    child processes running to finish processing whatever requests they
    might be handling at the time. It then starts up a new parent proxy
    server process and its children to handle new incoming requests.
    This allows zero-downtime upgrades with no impact to existing
    requests.
  </para>
  <para>
    The orphaned child processes may take a while to exit, depending on
    the length of the requests they were handling. However, sometimes an
    old process can be hung up due to some bug or hardware issue. In
    these cases, these orphaned processes will hang around forever.
    swift-orphans can be used to find and kill these orphans.
  </para>
  <para>
    swift-orphans with no arguments will just list the orphans it finds
    that were started more than 24 hours ago. You shouldn't really check
    for orphans until 24 hours after you perform a reload, as some
    requests can take a long time to process. swift-orphans -k TERM will
    send the SIG_TERM signal to the orphans processes, or you can kill
    -TERM the pids yourself if you prefer.
  </para>
  <para>
    You can run swift-orphans --help for more options.
  </para>
</section>
<section xml:id="swift-oldies">
  <title>Swift Oldies</title>
  <para>
    Swift Oldies are processes that have just been around for a long
    time. There's nothing necessarily wrong with this, but it might
    indicate a hung process if you regularly upgrade and reload/restart
    services. You might have so many servers that you don't notice when
    a reload/restart fails; swift-oldies can help with this.
  </para>
  <para>
    For example, if you upgraded and reloaded/restarted everything 2
    days ago, and you've already cleaned up any orphans with
    swift-orphans, you can run swift-oldies -a 48 to find any Swift
    processes still around that were started more than 2 days ago and
    then investigate them accordingly.
  </para>
</section>
<section xml:id="custom-log-handlers">
  <title>Custom Log Handlers</title>
  <para>
    Swift supports setting up custom log handlers for services by
    specifying a comma-separated list of functions to invoke when
    logging is setup. It does so via the log_custom_handlers
    configuration option. Logger hooks invoked are passed the same
    arguments as Swift's get_logger function (as well as the getLogger
    and LogAdapter object):
  </para>
  <informaltable>
    <tgroup cols="2">
      <colspec align="left" />
      <colspec align="left" />
      <thead>
        <row>
          <entry>
            Name
          </entry>
          <entry>
            Description
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            conf
          </entry>
          <entry>
            Configuration dict to read settings from
          </entry>
        </row>
        <row>
          <entry>
            name
          </entry>
          <entry>
            Name of the logger received
          </entry>
        </row>
        <row>
          <entry>
            log_to_console
          </entry>
          <entry>
            (optional) Write log messages to console on stderr
          </entry>
        </row>
        <row>
          <entry>
            log_route
          </entry>
          <entry>
            Route for the logging received
          </entry>
        </row>
        <row>
          <entry>
            fmt
          </entry>
          <entry>
            Override log format received
          </entry>
        </row>
        <row>
          <entry>
            logger
          </entry>
          <entry>
            The logging.getLogger object
          </entry>
        </row>
        <row>
          <entry>
            adapted_logger
          </entry>
          <entry>
            The LogAdapter object
          </entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
  <para>
    A basic example that sets up a custom logger might look like the
    following:
  </para>
  <programlisting language="python">
def my_logger(conf, name, log_to_console, log_route, fmt, logger,
              adapted_logger):
    my_conf_opt = conf.get('some_custom_setting')
    my_handler = third_party_logstore_handler(my_conf_opt)
    logger.addHandler(my_handler)
</programlisting>
  <para>
    See custom-logger-hooks-label for sample use cases.
  </para>
</section>
</section>
